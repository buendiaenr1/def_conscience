# def_conscience
Definir conciencia

## pca_leyenda
Lee archivos CSV, TXT, PDF para realizar el análisis de componentes principales.

Identificar relaciones conceptuales, no es una definición directa: El PCA no contiene en sí mismo las relaciones filosóficas (no es un sistema ontológico). No es un reflejo estadístico: captura cómo los conceptos filosóficos aparecen juntos en los textos analizados. Basado en co-ocurrencia: Si dos términos filosóficos (ej. “conscious” y artificial”) tienden a aparecer en los mismos contextos, el PCA los ubicará cerca.
Revelar relaciones filosóficas: proximidad de puntos, significa conceptos filosóficamente relacionadas. Distancia entre puntos, significa oposición o brecha filosófica. Agrupamiento (clusters), significa dominios filosóficos coherentes. Posición en ejes, significa dimensiones filosóficas subyacentes.
Mostrar relaciones específicas: relación de proximidad “information”+”conscious”, donde la relación filosófica, soporta la Teoría del Espacio de Trabajo Neuronal Global (Dahaene), que se interpreta como La conciencia como acceso global a la información. “emotion” + “learning”:  como base neurocientífica del aprendizaje afectivo (Damasio), que se interpreta como, las emociones como moduladoras esenciales de la cognición. Relaciones de distancia: “technology” vs “conscious” , donde su relación filosófica es, Brecha fenomenológica (Picard), que se interpreta como La IA puede simular, pero no experimentar; “artificial” vs “human” con relación filosófica, Límites de la IA en replicar la cognición humana, que se interpreta como La conciencia como propiedad emergente no computacional.
Agrupamientos temáticos, cluster neurocientífico (“brain”, “cortex”, “neural”): cuya relación filosófica: materialismo vs dualismo se interpreta como La conciencia como producto de procesos neuronales. Cluster educativo (“learning”,”teaching”,”student”), su relación filosófica es epistemología del aprendizaje, que se interpreta como se construye el conocimiento en sistemas naturales y artificiales.
Teniendo las siguientes limitaciones: no tiene profundidad filosófica, no captura matices argumentativos, solo captura asociaciones lineales, refleja lo escrito, no lo pensado.  


Identifying conceptual relationships is not a straightforward definition: PCA does not contain philosophical relationships (it is not an ontological system). It is not a statistical reflection: it captures how philosophical concepts appear together in the analyzed texts. Based on co-occurrence: If two philosophical terms (e.g., “conscious” and “artificial”) tend to appear in the same contexts, PCA will locate them close together.
Reveal philosophical relationships: Proximity of points means philosophically related concepts. Distance between points means opposition or philosophical gap. Clustering means coherent philosophical domains. Position on axes means underlying philosophical dimensions.
Show specific relationships: Proximity relationship “information” + “conscious,” where the philosophical relationship supports the Global Neural Workspace Theory (Dahaene), which is interpreted as consciousness as global access to information. “emotion” + “learning”: as the neuroscientific basis of affective learning (Damasio), which is interpreted as emotions as essential modulators of cognition. Distance relationships: “technology” vs. “conscious,” where their philosophical relationship is a Phenomenological Gap (Picard), which It is interpreted as: AI can simulate, but not experiment; "artificial" vs. "human" with a philosophical relationship, Limits of AI in replicating human cognition, which is interpreted as: Consciousness as a non-computational emergent property.
Thematic groupings, neuroscientific cluster ("brain," "cortex," "neural"): whose philosophical relationship: materialism vs. dualism is interpreted as: Consciousness as a product of neural processes. Educational cluster ("learning," "teaching," "student"), its philosophical relationship is: epistemology of learning, which is interpreted as how knowledge is constructed in natural and artificial systems.
It has the following limitations: it lacks philosophical depth, it does not capture argumentative nuances, it only captures linear associations, and it reflects what is written, not what is thought.


## des.py  

El problema en este caso es que se necesita ajustar cada resumen resumen a mano para ver de forma extensa dicho resumen, por lo que se creó la app en python para arreglar ese problema.
The problem in this case is that each summary needs to be adjusted manually in order to view it in full (shrink to unshrink), so the Python app was created to fix that problem.
Example: https://philpapers.org/s/intentionality%20AND%20ai    

### ...
Mindshaping and AI: Will Mindshaping a Robot Create an Artificial Person?John Dorsch - forthcoming - In Tad Zawidzki, Routledge Handbook of Mindshaping. pp. 406-417.
This chapter examines possible ramifications of mindshaping a social robot. It explores how such an agent might learn to represent psychological states, align its behavior with evolving societal norms, and develop capacities for self-directed mindreading and normative self-knowledge. Integrating perspectives from cultural evolution and naturalized intentionality, this approach suggests that social robots could achieve a level of norm-based self-regulation typically reserved for humans, fulfilling criteria for moral and legal personhood. However, this possibility raises ethical concerns: creating a self-knowing agent (...)
### ...
Mindshaping and AI: Will Mindshaping a Robot Create an Artificial Person?John Dorsch - forthcoming - In Tad Zawidzki, Routledge Handbook of Mindshaping. pp. 406-417.
This chapter examines possible ramifications of mindshaping a social robot. It explores how such an agent might learn to represent psychological states, align its behavior with evolving societal norms, and develop capacities for self-directed mindreading and normative self-knowledge. Integrating perspectives from cultural evolution and naturalized intentionality, this approach suggests that social robots could achieve a level of norm-based self-regulation typically reserved for humans, fulfilling criteria for moral and legal personhood. However, this possibility raises ethical concerns: creating a self-knowing agent would tax care-giving resources as we would need to provide AI welfare, thus undermining our capacity to act responsibly toward humans, non-human animals, and the environment, to whom our moral consideration is already owed and in desperate need. Thus, this chapter concludes by urging caution, warning that attempts to cultivate moral responsibility in artificial agents may have destabilizing consequences for moral practices. (shrink)

## urls.txt
https://philpapers.org/s/phenomenology%20AND%20machine%20learning
https://philpapers.org/s/qualia%20AND%20computational%20models
https://philpapers.org/s/emotion%20AND%20neuroeducation
https://philpapers.org/s/global%20neuronal%20Workspace%20AND%20simulation
https://philpapers.org/s/embodied%20cognition%20AND%20ai%20limitations
https://philpapers.org/s/theory%20of%20Mind%20AND%20machine%20consciousness
https://philpapers.org/s/neuroplasticity%20AND%20emotional%20modulation
https://philpapers.org/s/affective%20computing%20AND%20phenomenological%20gap
https://philpapers.org/s/intentionality%20AND%20ai

